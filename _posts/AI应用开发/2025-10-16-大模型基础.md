---
title: 大模型基础
categories: AI应用开发
---
* 
{:toc levels="2..4"}

## 大模型核心架构与机制

| 架构 / 机制 |	提出时间 |	核心机制简述 |	典型模型例子 |	适应场景 |	优点|	缺点|
| --- | --- | --- | --- | --- |
| RNN（循环神经网络）	|1982 年|	循环连接构建隐状态，传递序列上下文记忆，适配变长输入。	|Elman RNN、LSTM、GRU	|短文本处理、语音识别、简单时序预测	|结构简洁、适配序列数据、训练 / 部署轻量化	|无法并行、易梯度消失 / 爆炸、长依赖捕捉弱|
| CNN（卷积神经网络）	|1989 年	|卷积核提取局部特征 + 池化降维，拓展至文本局部语义提取。	|TextCNN、CNN-DM	|短文本分类、情感分析、关键词提取、图像识别	|并行度高、训练快、局部特征提取稳定	|窗口固定、难捕全局依赖、无文本时序感知|
| GAN（生成对抗网络）	|2014 年	|生成器与判别器博弈训练，逼近真实数据分布优化生成效果。	|DCGAN、CycleGAN、SeqGAN	|图像生成、风格迁移、数据增强、对话优化	|生成样本逼真、细节丰富、适配无监督生成	|训练不稳定、易模式崩溃、离散文本训练难|
| VAE（变分自编码器）	|2013 年	|概率编码器 - 解码器结构，映射数据至潜在空间实现生成与重构。	|β-VAE、CVAE、VAE-GAN	|降维、异常检测、可控生成、个性化推荐| 训练稳定、可解释性强、支持无监督学习	|生成清晰度低、重构丢细节、高维数据效率低|
| Transformer（LLM 核心）	|2017 年	|以自注意力、多头注意力、位置编码为核心，并行处理序列，摒弃循环结构。	|GPT 系列、BERT、LLaMA、ChatGPT	|翻译、文本生成、对话、代码生成、推理	|长依赖捕捉强、并行高效、泛化能力出色|参数量大、训练 / 推理成本高、存在幻觉与偏见|


## 架构机制的演化

<img src="/assets/images/演化.png" style="width:100%" alt="演化图">

## RNN模型的理解


 >RNN 就像一个带着记事本做事的人，每做下一步时，都会参考一下刚才记在本子上的内容。

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ===================== 第一步：准备数据（新手简化版） =====================
# 模拟短文本数据：每条文本转成数字序列（实际项目需用Tokenizer，这里简化）
# 文本：["我喜欢这个电影", "这个电影超烂", "剧情很棒", "太难看了"]
# 标签：1=正面，0=负面
texts = [[1, 2, 3, 4], [3, 4, 5, 6], [7, 8], [9, 10]]  # 数字编码后的文本
labels = [1, 0, 1, 0]  # 对应情感标签

# 数据预处理：统一序列长度（RNN需要固定输入长度，短的补0）
max_len = 4
inputs = []
for text in texts:
    # 补0：比如[7,8]变成[7,8,0,0]
    padded = text + [0]*(max_len - len(text))
    inputs.append(padded)

# 转成PyTorch张量（模型能处理的格式）
inputs = torch.tensor(inputs, dtype=torch.long)  # 输入：[4,4]（4条数据，每条4个词）
labels = torch.tensor(labels, dtype=torch.float)  # 标签：[4]

# ===================== 第二步：定义RNN模型 =====================
class SimpleRNN(nn.Module):
    def __init__(self, vocab_size=11, embed_dim=8, hidden_dim=16):
        super().__init__()
        # 1. 词嵌入：把数字编码转成有意义的向量（比如"电影"转成[0.2,0.5,...]）
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        # 2. 核心RNN层：输入是词向量，输出是隐藏状态
        self.rnn = nn.RNN(
            input_size=embed_dim,  # 每个词的向量长度
            hidden_size=hidden_dim, # 隐藏层维度（记忆的容量）
            batch_first=True,       # 输入格式：[样本数, 序列长度, 向量维度]（新手友好）
        )
        # 3. 输出层：把RNN的输出转成情感分类结果（0/1）
        self.fc = nn.Linear(hidden_dim, 1)  # 从16维隐藏状态转成1维（是否正面）

    def forward(self, x):
        # 步骤1：词嵌入 → [4,4] → [4,4,8]
        x_embed = self.embedding(x)
        # 步骤2：RNN处理 → 输出：[4,4,16]，最后一步隐藏状态：[1,4,16]
        rnn_out, hidden = self.rnn(x_embed)
        # 步骤3：取最后一步的隐藏状态（代表整句话的语义）→ [4,16]
        last_hidden = hidden.squeeze(0)  # 去掉多余维度
        # 步骤4：分类输出 → [4,1]
        output = self.fc(last_hidden)
        # 转成概率（0-1之间）
        output = torch.sigmoid(output)
        return output

# ===================== 第三步：训练模型（使用RNN） =====================
# 初始化模型、损失函数、优化器
model = SimpleRNN()
criterion = nn.BCELoss()  # 二分类损失函数
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 训练30轮（新手看效果即可）
for epoch in range(30):
    # 清零梯度
    optimizer.zero_grad()
    # 前向传播：输入数据，得到预测结果
    outputs = model(inputs).squeeze(1)  # 去掉多余维度 → [4]
    # 计算损失（预测值和真实标签的差距）
    loss = criterion(outputs, labels)
    # 反向传播：更新模型参数
    loss.backward()
    optimizer.step()

    # 打印训练过程
    if (epoch+1) % 10 == 0:
        print(f"第{epoch+1}轮，损失值：{loss.item():.4f}")

# ===================== 第四步：使用训练好的RNN做预测 =====================
# 测试数据：["剧情超棒"] → 编码为[7,8,0,0]
test_input = torch.tensor([[7,8,0,0]], dtype=torch.long)
# 预测
with torch.no_grad():  # 预测时不计算梯度，节省资源
    pred = model(test_input)
    pred_label = 1 if pred > 0.5 else 0

print(f"\n预测结果：{'正面情感' if pred_label == 1 else '负面情感'}，概率：{pred.item():.4f}")

```

## CNN模型的理解

> CNN 在文本任务中，核心是提取局部特征（比如 “超好吃”“太难看” 这类关键短语）

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ===================== 第一步：准备数据（和RNN完全一致） =====================
# 模拟短文本数据：数字编码后的文本 + 情感标签（1=正面，0=负面）
texts = [[1, 2, 3, 4], [3, 4, 5, 6], [7, 8], [9, 10]]  # ["我喜欢这个电影", "这个电影超烂", "剧情很棒", "太难看了"]
labels = [1, 0, 1, 0]

# 统一序列长度（补0），转成张量
max_len = 4
inputs = []
for text in texts:
    padded = text + [0]*(max_len - len(text))
    inputs.append(padded)
inputs = torch.tensor(inputs, dtype=torch.long)  # [4,4]（4条数据，每条4个词）
labels = torch.tensor(labels, dtype=torch.float)  # [4]

# ===================== 第二步：定义CNN模型（核心差异） =====================
class SimpleTextCNN(nn.Module):
    def __init__(self, vocab_size=11, embed_dim=8, num_filters=16):
        super().__init__()
        # 1. 词嵌入：和RNN完全一样，把数字转成有意义的向量
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # 2. 核心CNN层：用不同尺寸的卷积核提取局部特征（比如2个词、3个词的短语）
        # 卷积核尺寸：(kernel_size, embed_dim) → 对词向量的列（维度）全卷积，只在行（序列）上滑动
        self.conv1 = nn.Conv2d(
            in_channels=1,          # 输入通道数：文本只有1个通道（类似灰度图）
            out_channels=num_filters, # 卷积核数量（提取16种局部特征）
            kernel_size=(2, embed_dim) # 卷积核尺寸：2个词 + 词向量维度（8）
        )
        self.conv2 = nn.Conv2d(
            in_channels=1,
            out_channels=num_filters,
            kernel_size=(3, embed_dim) # 卷积核尺寸：3个词 + 词向量维度
        )
        
        # 3. 池化层：提取每个卷积结果的最大值（保留最关键的局部特征）
        self.pool = nn.AdaptiveMaxPool1d(1)
        
        # 4. 输出层：把卷积+池化后的特征转成分类结果
        self.fc = nn.Linear(num_filters * 2, 1)  # 2种卷积核，各输出16维 → 32维

    def forward(self, x):
        # 步骤1：词嵌入 → [4,4] → [4,4,8]
        x_embed = self.embedding(x)
        # 步骤2：增加通道维度 → [4,1,4,8]（适配CNN的输入格式：批量数×通道数×序列长度×词向量维度）
        x_embed = x_embed.unsqueeze(1)
        
        # 步骤3：卷积+激活（提取局部特征）
        # conv1输出：[4,16,3,1] → squeeze后[4,16,3]；conv2输出：[4,16,2,1] → squeeze后[4,16,2]
        conv1_out = torch.relu(self.conv1(x_embed)).squeeze(3)
        conv2_out = torch.relu(self.conv2(x_embed)).squeeze(3)
        
        # 步骤4：池化（保留每个特征的最大值）→ 都变成[4,16,1] → squeeze后[4,16]
        pool1_out = self.pool(conv1_out).squeeze(2)
        pool2_out = self.pool(conv2_out).squeeze(2)
        
        # 步骤5：拼接两种卷积核的特征 → [4,32]
        concat = torch.cat([pool1_out, pool2_out], dim=1)
        
        # 步骤6：分类输出 → [4,1]，转成0-1概率
        output = self.fc(concat)
        output = torch.sigmoid(output)
        return output

# ===================== 第三步：训练模型（和RNN几乎一致） =====================
model = SimpleTextCNN()
criterion = nn.BCELoss()  # 同样用二分类损失函数
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 训练30轮
for epoch in range(30):
    optimizer.zero_grad()
    outputs = model(inputs).squeeze(1)  # 去掉多余维度 → [4]
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f"第{epoch+1}轮，损失值：{loss.item():.4f}")

# ===================== 第四步：预测（和RNN完全一致） =====================
# 测试数据：["剧情超棒"] → 编码为[7,8,0,0]
test_input = torch.tensor([[7,8,0,0]], dtype=torch.long)
with torch.no_grad():
    pred = model(test_input)
    pred_label = 1 if pred > 0.5 else 0

print(f"\n预测结果：{'正面情感' if pred_label == 1 else '负面情感'}，概率：{pred.item():.4f}")
```

## GAN模型的理解

> GAN 主打 “逼真生成”，文本场景常用：短文本生成、对话回复优化、数据增强；图像场景更主流（如人脸生成），这里聚焦文本生成。GAN训练不稳定，需平衡生成器 / 判别器的损失（一方太强会导致模式崩溃）；
文本是离散数据：GAN 天生适配连续数据（图像），文本需用 “硬采样”（argmax），梯度易断裂（进阶用 Gumbel-Softmax）；
简化版仅做演示：真实 SeqGAN 需结合强化学习，这里用 RNN+CNN 简化。

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# ===================== 第一步：准备数据（简单文本序列） =====================
# 目标：生成类似["我喜欢电影", "电影超好看"]的短文本，先编码成数字序列
# 词汇表：我(1), 喜欢(2), 电影(3), 超(4), 好看(5), 烂(6), 填充(0)
vocab_size = 7
max_len = 3  # 生成3个词的短文本
# 真实样本（正面情感短文本）
real_samples = torch.tensor([[1,2,3], [3,4,5]], dtype=torch.long)  # ["我喜欢电影", "电影超好看"]

# ===================== 第二步：定义GAN（生成器+判别器） =====================
# 生成器：随机噪声 → 生成假文本序列
class Generator(nn.Module):
    def __init__(self, noise_dim=10, hidden_dim=16):
        super().__init__()
        self.fc1 = nn.Linear(noise_dim, hidden_dim * max_len)
        self.rnn = nn.RNN(hidden_dim, vocab_size, batch_first=True)  # 用RNN生成序列

    def forward(self, noise):
        # 噪声：[批量, 10] → 映射成[批量, 3, 16]
        x = self.fc1(noise).reshape(-1, max_len, 16)
        # RNN生成序列概率分布：[批量, 3, 7]
        out, _ = self.rnn(x)
        return torch.softmax(out, dim=-1)  # 转成概率

# 判别器：判断文本是真实/生成的（二分类）
class Discriminator(nn.Module):
    def __init__(self, vocab_size=7, embed_dim=8, hidden_dim=16):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.cnn = nn.Conv1d(embed_dim, hidden_dim, kernel_size=2)  # CNN提取局部特征
        self.fc = nn.Linear(hidden_dim * 2, 1)  # 池化后维度

    def forward(self, x):
        # 文本编码：[批量, 3] → [批量, 3, 8] → 转置适配CNN：[批量, 8, 3]
        x_embed = self.embedding(x).permute(0,2,1)
        # CNN+池化：[批量, 16, 2] → 最大值池化→[批量,16]
        cnn_out = torch.relu(self.cnn(x_embed))
        pool_out = torch.max(cnn_out, dim=2)[0]
        # 二分类输出：[批量,1]，0=假，1=真
        out = self.fc(pool_out)
        return torch.sigmoid(out)

# ===================== 第三步：训练GAN（核心是对抗训练） =====================
# 初始化模型、优化器、损失函数
noise_dim = 10
generator = Generator(noise_dim)
discriminator = Discriminator()
g_optimizer = optim.Adam(generator.parameters(), lr=0.001)
d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)
criterion = nn.BCELoss()

# 训练50轮（GAN训练需更多轮次）
for epoch in range(50):
    # ---------------------- 1. 训练判别器：区分真假样本 ----------------------
    d_optimizer.zero_grad()
    # 生成假样本：随机噪声→概率分布→取最大概率的词
    noise = torch.randn(2, noise_dim)  # 2个样本，和真实样本数量一致
    fake_probs = generator(noise)
    fake_samples = torch.argmax(fake_probs, dim=-1)  # 转成数字序列

    # 判别真实样本（标签=1）
    real_pred = discriminator(real_samples)
    real_loss = criterion(real_pred, torch.ones_like(real_pred))
    # 判别假样本（标签=0）
    fake_pred = discriminator(fake_samples)
    fake_loss = criterion(fake_pred, torch.zeros_like(fake_pred))
    # 总判别损失
    d_loss = (real_loss + fake_loss) / 2
    d_loss.backward()
    d_optimizer.step()

    # ---------------------- 2. 训练生成器：骗过判别器 ----------------------
    g_optimizer.zero_grad()
    # 重新生成假样本（避免梯度共享）
    noise = torch.randn(2, noise_dim)
    fake_probs = generator(noise)
    fake_samples = torch.argmax(fake_probs, dim=-1)
    # 生成器希望判别器把假样本判为真（标签=1）
    g_pred = discriminator(fake_samples)
    g_loss = criterion(g_pred, torch.ones_like(g_pred))
    g_loss.backward()
    g_optimizer.step()

    # 打印进度
    if (epoch+1) % 10 == 0:
        print(f"第{epoch+1}轮 | 判别器损失：{d_loss.item():.4f} | 生成器损失：{g_loss.item():.4f}")

# ===================== 第四步：用GAN生成文本 =====================
# 词汇映射：数字→文字
vocab = {0:"", 1:"我", 2:"喜欢", 3:"电影", 4:"超", 5:"好看", 6:"烂"}
# 生成新样本
with torch.no_grad():
    noise = torch.randn(1, noise_dim)
    fake_probs = generator(noise)
    fake_sample = torch.argmax(fake_probs, dim=-1).squeeze().tolist()
    # 转成文字
    gen_text = "".join([vocab[i] for i in fake_sample])
    print(f"\n生成的短文本：{gen_text}")
```

## VAE模型的理解

> VAE 主打 “可控生成 / 概率建模”，常用：带条件的文本生成、异常检测、数据降维。

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ===================== 第一步：准备数据（同GAN） =====================
vocab_size = 7
max_len = 3
# 真实样本：["我喜欢电影", "电影超好看"]
real_samples = torch.tensor([[1,2,3], [3,4,5]], dtype=torch.long)
# 条件标签：1=正面情感（控制生成正面文本）
cond_labels = torch.tensor([[1], [1]], dtype=torch.float)

# ===================== 第二步：定义VAE（编码器+解码器） =====================
class VAE(nn.Module):
    def __init__(self, vocab_size=7, embed_dim=8, latent_dim=5, cond_dim=1):
        super().__init__()
        # 编码器：文本+条件 → 潜在空间（均值+方差）
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.enc_fc1 = nn.Linear((embed_dim + cond_dim) * max_len, 32)
        self.enc_mu = nn.Linear(32, latent_dim)  # 潜在空间均值
        self.enc_logvar = nn.Linear(32, latent_dim)  # 潜在空间方差对数

        # 解码器：潜在空间+条件 → 生成文本
        self.dec_fc1 = nn.Linear(latent_dim + cond_dim, 32)
        self.dec_fc2 = nn.Linear(32, vocab_size * max_len)

    def encode(self, x, cond):
        # 文本+条件拼接：[2,3,8] + [2,1] → 扩展后[2,3,1] → 拼接[2,3,9] → 展平[2,27]
        x_embed = self.embedding(x)
        cond_expand = cond.unsqueeze(1).repeat(1, max_len, 1)
        concat = torch.cat([x_embed, cond_expand], dim=-1).flatten(1)
        # 映射到潜在空间
        h = torch.relu(self.enc_fc1(concat))
        return self.enc_mu(h), self.enc_logvar(h)

    def reparameterize(self, mu, logvar):
        # 重参数化：避免直接采样导致梯度消失
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z, cond):
        # 潜在空间+条件拼接：[2,5] + [2,1] → [2,6]
        concat = torch.cat([z, cond], dim=-1)
        # 生成文本概率：[2,6] → [2,32] → [2,21]（3*7）→ 重塑[2,3,7]
        h = torch.relu(self.dec_fc1(concat))
        out = self.dec_fc2(h).reshape(-1, max_len, vocab_size)
        return torch.softmax(out, dim=-1)

    def forward(self, x, cond):
        mu, logvar = self.encode(x, cond)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z, cond)
        return recon, mu, logvar

# ===================== 第三步：训练VAE =====================
# 损失函数：重构损失 + KL散度（约束潜在空间分布）
def vae_loss(recon_probs, x, mu, logvar):
    # 重构损失：生成文本和真实文本的交叉熵
    recon_loss = nn.CrossEntropyLoss()(recon_probs.reshape(-1, vocab_size), x.flatten())
    # KL散度：让潜在空间接近标准正态分布
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + 0.001 * kl_loss  # 平衡系数

# 初始化模型和优化器
vae = VAE()
optimizer = optim.Adam(vae.parameters(), lr=0.001)

# 训练100轮
for epoch in range(100):
    optimizer.zero_grad()
    recon_probs, mu, logvar = vae(real_samples, cond_labels)
    loss = vae_loss(recon_probs, real_samples, mu, logvar)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 20 == 0:
        print(f"第{epoch+1}轮，损失值：{loss.item():.4f}")

# ===================== 第四步：可控生成文本 =====================
vocab = {0:"", 1:"我", 2:"喜欢", 3:"电影", 4:"超", 5:"好看", 6:"烂"}
# 控制生成“正面情感”文本（cond=1）
with torch.no_grad():
    # 从标准正态分布采样潜在向量
    z = torch.randn(1, 5)
    cond = torch.tensor([[1]], dtype=torch.float)  # 正面情感条件
    recon_probs = vae.decode(z, cond)
    gen_sample = torch.argmax(recon_probs, dim=-1).squeeze().tolist()
    gen_text = "".join([vocab[i] for i in gen_sample])
    print(f"\n可控生成的文本（正面情感）：{gen_text}")

```

## Transformer模型的理解

>位置编码是必需：Transformer 无循环 / 卷积，无法感知序列顺序，需手动加位置信息；
自注意力是核心：可直接计算任意词的关联，捕捉长依赖；
残差连接 + 层归一化：防止梯度消失，是 Transformer 稳定训练的关键。

```python
import torch
import torch.nn as nn
import torch.optim as optim
import math

# ===================== 第一步：准备数据（同RNN/CNN） =====================
texts = [[1, 2, 3, 4], [3, 4, 5, 6], [7, 8], [9, 10]]  
# ["我喜欢这个电影", "这个电影超烂", "剧情很棒", "太难看了"]
labels = [1, 0, 1, 0]
max_len = 4
# 补0+转张量
inputs = []
for text in texts:
    padded = text + [0]*(max_len - len(text))
    inputs.append(padded)
inputs = torch.tensor(inputs, dtype=torch.long)  # [4,4]
labels = torch.tensor(labels, dtype=torch.float)  # [4]

# ===================== 第二步：定义Transformer模型（简化版） =====================
class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size=11, embed_dim=8, num_heads=2, hidden_dim=16):
        super().__init__()
        # 1. 词嵌入 + 位置编码（Transformer无循环，需手动加位置信息）
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoding = self._get_pos_encoding(max_len, embed_dim)

        # 2. 多头自注意力层（核心）
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        
        # 3. 前馈网络
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embed_dim)
        )
        
        # 4. 输出层
        self.fc = nn.Linear(embed_dim, 1)

    def _get_pos_encoding(self, max_len, embed_dim):
        # 位置编码：给每个位置生成唯一向量
        pos = torch.arange(max_len).unsqueeze(1)
        div = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0)/embed_dim))
        pos_enc = torch.zeros(max_len, embed_dim)
        pos_enc[:, 0::2] = torch.sin(pos * div)
        pos_enc[:, 1::2] = torch.cos(pos * div)
        return pos_enc

    def forward(self, x):
        # 步骤1：词嵌入 + 位置编码 → [4,4,8]
        x_embed = self.embedding(x) + self.pos_encoding.to(x.device)
        
        # 步骤2：自注意力（加掩码，忽略填充的0）
        mask = (x == 0)  # 填充位掩码：[4,4]
        attn_out, _ = self.attention(x_embed, x_embed, x_embed, key_padding_mask=mask)
        
        # 步骤3：残差连接 + 层归一化
        x = x_embed + attn_out
        x = nn.LayerNorm(embed_dim).forward(x)
        
        # 步骤4：前馈网络 + 残差连接 + 层归一化
        ffn_out = self.ffn(x)
        x = x + ffn_out
        x = nn.LayerNorm(embed_dim).forward(x)
        
        # 步骤5：取第一个token的输出（或平均）做分类 → [4,8]
        x = x[:, 0, :]
        
        # 步骤6：分类输出 → [4,1]
        output = self.fc(x)
        output = torch.sigmoid(output)
        return output

# ===================== 第三步：训练模型 =====================
model = SimpleTransformer()
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练30轮
for epoch in range(30):
    optimizer.zero_grad()
    outputs = model(inputs).squeeze(1)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f"第{epoch+1}轮，损失值：{loss.item():.4f}")

# ===================== 第四步：预测 =====================
test_input = torch.tensor([[7,8,0,0]], dtype=torch.long)  # ["剧情超棒"]
with torch.no_grad():
    pred = model(test_input)
    pred_label = 1 if pred > 0.5 else 0
    print(f"\n预测结果：{'正面情感' if pred_label == 1 else '负面情感'}，概率：{pred.item():.4f}")
```


## 术语说明

1. 归一化（Normalization）

    ```txt
    一句话解释：
    把数据 “调整到同一尺度”，让模型更容易学习。
    通俗比喻：
    你要比较两个学生的成绩：A 考了 85/100B 考了 45/50如果不做归一化，你会觉得 A 更高，但其实 B 是 90%。
    归一化就是把它们都 “换算成同一套标准”，比如都转成 0~1 或均值为 0、方差为 1。
    在神经网络中的作用：

        避免某些特征因为数值太大而 “主导” 训练
        让梯度更稳定，训练更快
        防止模型 “跑偏”

    常见类型：

        Batch Normalization（按批次归一化）
        Layer Normalization（按层归一化，Transformer 常用）
    ```

2. 残差（Residual）  

    ```txt
    一句话解释：
    让网络 “学会差多少”，而不是 “学会全部”，从而更容易训练深层网络。
    通俗比喻：
    你要从 1 数到 100。如果直接让你 “记住从 1 到 100 的所有数”，很难。但如果让你 “记住每次加 1”，就很容易。
    残差就是这个 “加 1” 的部分。
    在神经网络中的作用：

        解决深层网络梯度消失问题
        让网络可以轻松 “跳过” 不重要的层
        让训练更深的网络成为可能（比如 ResNet 可以做到 100 层以上）

    典型结构：
    plaintext

    输出 = 输入 + 网络学习到的残差
    ```

3. 自注意力（Self-Attention）  

    ```txt
    一句话解释：
    让每个词都能 “看” 到句子中其他所有词，从而理解它们之间的关系。
    通俗比喻：你在读句子：“他告诉小明，他明天不来了。”你知道第二个 “他” 指的是 
    “他”，不是 “小明”。自注意力就是让模型也能做到这一点：
    每个词都会给其他词分配一个 “关注度”，表示它们之间的关联强度。
    在 Transformer 中的作用：

        捕捉长距离依赖（比如句子开头和结尾的关系）
        并行计算，比 RNN 快得多
        是现代 LLM 的核心机制

    关键特点：

        多头注意力：多个 “注意力视角” 同时看
        可以处理任意长度的序列
        不依赖循环或卷积
    ```

4. 卷积（Convolution）  

    ```txt
    一句话解释：
    用一个 “小窗口” 在数据上滑动，提取局部特征。
    通俗比喻：你在看一张照片，你不会一次性看整张，而是用眼睛的 “小视野” 一点点扫。
    卷积就是这个 “小视野”，它在图像或文本上滑动，提取边缘、纹理、短语等局部特征。
    在图像中的作用：

        提取边缘、线条、形状等特征
        越深层的卷积越能提取复杂特征（如眼睛、鼻子）

    在文本中的作用：

        提取 n-gram 短语特征（比如 “超好吃”“太难看”）
        用不同大小的卷积核捕捉不同长度的局部模式
    ```

5. 池化（Pooling）  

    ```txt
    一句话解释：
    把提取到的局部特征 “压缩”，只保留最重要的信息。
    通俗比喻：你看完一段文章，不会记住每一个字，只会记住 “重点”。
    池化就是帮你 “记住重点”，去掉无关紧要的细节。
    常见类型：

        最大池化（Max Pooling）：只保留每个窗口中最大的值（最显著的特征）
        平均池化（Average Pooling）：取平均值（整体趋势）

    在 CNN 中的作用：
    降低特征维度，减少计算量
    防止过拟合
    让模型更关注 “有没有某个特征”，而不是 “特征的精确位置”
    ```
